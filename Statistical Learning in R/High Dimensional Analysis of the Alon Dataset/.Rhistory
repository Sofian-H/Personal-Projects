folds.x.train <- folds.x[folds != fold,]
folds.x.test <- folds.x[folds == fold,]
folds.y.train <- dat[folds != fold,2001]
folds.y.test <- dat[folds == fold,2001]
folds.fit <- glmnet(folds.x.train, folds.y.train, alpha=1, family="binomial",lambda=lasso$lambda.min)
folds.fit.test <- predict(folds.fit, folds.x.test, type="class")
print(apply(folds.fit.test != y, 2, sum))
errors[fold+1] <- apply(folds.fit.test != y, 2, sum)
}
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
errors <- c(1:62)*0
folds<-c(1:62)
for (fold in 0:61) {
print(paste("fold", fold))
folds.x <- model.matrix(y~x, data=dat)
folds.x.train <- folds.x[folds != fold,]
folds.x.test <- folds.x[folds == fold,]
folds.y.train <- dat[folds != fold,2001]
folds.y.test <- dat[folds == fold,2001]
folds.fit <- glmnet(folds.x.train, folds.y.train, alpha=1, family="binomial",lambda=lasso$lambda.min)
folds.fit.test <- predict(folds.fit, folds.x.test, type="class")
truc<-apply(folds.fit.test != y, 2, sum)
errors[fold+1] <- apply(folds.fit.test != y, 2, sum)
}
truc
class(truc)
truc+1
folds.fit.test
folds.fit
predict(folds.fit,folds.fit.test)
predict(folds.fit,folds.fit.test,type="class")
folds.x.test
folds.fit
predict(folds.fit,folds.fit.test,type="class")
folds.fit.test
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
errors <- c(1:62)*0
folds<-c(1:62)
for (fold in 0:61) {
print(paste("fold", fold))
folds.x <- model.matrix(y~x, data=dat)
folds.x.train <- folds.x[folds != fold,]
folds.x.test <- folds.x[folds == fold,]
folds.y.train <- dat[folds != fold,2001]
folds.y.test <- dat[folds == fold,2001]
folds.fit <- glmnet(folds.x.train, folds.y.train, alpha=1, family="binomial",lambda=lasso$lambda.min)
folds.fit.test <- predict(folds.fit, folds.x.test,s=lasso$lambda.min, type="class")
errors[fold+1] <- apply(folds.fit.test != y, 2, sum)
}
folds.fit.test
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
errors <- c(1:62)*0
folds<-c(1:62)
predictions <- c(1:62)*0
for (fold in 0:61) {
print(paste("fold", fold))
folds.x <- model.matrix(y~x, data=dat)
folds.x.train <- folds.x[folds != fold,]
folds.x.test <- folds.x[folds == fold,]
folds.y.train <- dat[folds != fold,2001]
folds.y.test <- dat[folds == fold,2001]
folds.fit <- glmnet(folds.x.train, folds.y.train, alpha=1, family="binomial",lambda=lasso$lambda.min)
folds.fit.test <- predict(folds.fit, folds.x.test,s=lasso$lambda.min, type="class")
}
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
errors <- c(1:62)*0
folds<-c(1:62)
predictions <- c(1:62)*0
for (fold in 0:61) {
print(paste("fold", fold))
folds.x <- model.matrix(y~x, data=dat)
folds.x.train <- folds.x[folds != fold,1:2000]
folds.x.test <- folds.x[folds == fold,1:2000]
folds.y.train <- dat[folds != fold,2001]
folds.y.test <- dat[folds == fold,2001]
folds.fit <- glmnet(folds.x.train, folds.y.train, alpha=1, family="binomial",lambda=lasso$lambda.min)
folds.fit.test <- predict(folds.fit, folds.x.test,s=lasso$lambda.min, type="class")
}
folds.x
dim(folds.x)
folds
folds.i
folds.x
dat[c(1,2,3) != 2,2001]
dat[c(1,2,3),2001]
c(1,2,3,-1)
dat[-c(2:62),2001]
dat[1,62]
dat[1,2001]
dat[-2,2001]
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(x_train,y_train,alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,x_test,s=lasso$lambda.min,type="class")
}
for (k in 1:62) {
print(k)
x_train<-dat[-folds[k],1:2000]
print("a")
x_test <-dat[folds[k],1:2000]
print("a")
y_train<-dat[-folds[k],2001]
print("a")
y_test<-dat[folds[k],2001]
print("a")
fit<-glmnet(x_train,y_train,alpha=1,family="binomial",lambda=lasso$lambda.min)
print("a")
pred<-predict(fit,x_test,s=lasso$lambda.min,type="class")
}
for (k in 1:62) {
print(k)
x_train<-dat[-folds[k],1:2000]
print("a")
x_test <-dat[folds[k],1:2000]
print("a")
y_train<-dat[-folds[k],2001]
print("a")
y_test<-dat[folds[k],2001]
print("a")
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
print("a")
pred<-predict(fit,x_test,s=lasso$lambda.min,type="class")
}
for (k in 1:62) {
print(k)
x_train<-dat[-folds[k],1:2000]
print("a")
x_test <-dat[folds[k],1:2000]
print("a")
y_train<-dat[-folds[k],2001]
print("a")
y_test<-dat[folds[k],2001]
print("a")
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
print("a")
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
}
dat$y
rm(errors)
y
overall_error<-0
tumour_error<-0
normal-error<-0
normal_error<-0
X<-load("..\\alon.rda")
library(MASS)
dat<-vector()
for (i in 1:2000) {
dat<-cbind(dat,x[,i])
}
dat<-cbind(dat,y)
dat<-as.data.frame(dat)
C<-cov(x)
lin_discr<-lda(formula=y~x)
write.csv(lin_discr$means,file="LDA means.csv")
write.csv(C,file="cov.csv")
library(glmnet)
set.seed(7)
lasso<-cv.glmnet(x,y,alpha=1,family="binomial",nfolds = 62)
log_reg<-glmnet(x,y,alpha=1,family="binomial",lambda=lasso$lambda.min)
#To estimate CV errors
library(ipred)
# force predict to return class labels only
mypredict.lda <- function(object, newdata)
predict(object, newdata = newdata)$class
err_lda<-errorest(data=dat,y~x,model=lda,estimator="cv",predict=mypredict.lda,
est.para=control.errorest(k=62))
mypredict.logreg <- function(object, newdata)
predict.glmnet(object,newdata = newdata)$class
err_lda<-errorest(data=dat,y~x,model=lda,estimator="cv",predict=mypredict.lda,
est.para=control.errorest(,predictions=TRUE))
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
overall_error<-0
tumour_error<-0
normal_error<-0
folds<-c(1:62)
predictions <- c(1:62)*0
"cours"=="lab"
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
overall_error<-0
tumour_error<-0
normal_error<-0
folds<-c(1:62)
predictions <- c(1:62)*0
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k])
tumour_error<-tumour_error+1
}
else {
if (pred!=dat$y[k])
normal_error<-normal_error+1
}
}
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k]) {
tumour_error<-tumour_error+1
overall_error<-overall_error+1
}
}
else {
if (pred!=dat$y[k]) {
normal_error<-normal_error+1
overall_error<-overall_error+1
}
}
}
overall_error
tumour_error
normal_error
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k]) {
tumour_error<-tumour_error+1
}
}
else {
if (pred!=dat$y[k]) {
normal_error<-normal_error+1
}
}
}
tumour_error
normal_error
overall_error<-0
tumour_error<-0
normal_error<-0
folds<-c(1:62)
predictions <- c(1:62)*0
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k]) {
tumour_error<-tumour_error+1
}
}
else {
if (pred!=dat$y[k]) {
normal_error<-normal_error+1
}
}
}
tumour_error
normal_error
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
overall_error<-0
tumour_error<-0
normal_error<-0
folds<-c(1:62)
predictions <- c(1:62)*0
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k]) {
tumour_error<-tumour_error+1
overall_error<-overall_error+1
}
}
else {
if (pred!=dat$y[k]) {
normal_error<-normal_error+1
overall_error<-overall_error+1
}
}
}
tumour_error
normal_error
overall_error
overall_error<-overall_error/62
tumour_error<-tumour_error/40
normal_error<-normal_error/22
overall_error
tumour_error
normal_error
install.packages("TANDEM")
library(TANDEM)
cv_glmnet = nested.cv(x, y, method="glmnet", alpha=0.5)
as.factor(y)
cv_glmnet = nested.cv(x, dat$y, method="glmnet", alpha=0.5)
dat$upstream
cv_glmnet = nested.cv(x, dat$y, upsteam=dat$upstream method="glmnet", alpha=0.5)
cv_glmnet = nested.cv(x, dat$y, upsteam=dat$upstream, method="glmnet", alpha=0.5)
c("TRUE")*15
upstream<-vector(length=2000)
upstream<-vector(length=2000)
for (i in 1:2000)
upstream[i]<-TRUE
cv_glmnet = nested.cv(x, dat$y, upstream=usptream, method="glmnet", alpha=0.5)
upstream<-vector(length=2000)
upstream
for (i in 1:2000) {
upstream[i]<-TRUE
}
upstream
cv_glmnet = nested.cv(x, dat$y, upstream=usptream, method="glmnet", alpha=0.5)
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", alpha=0.5)
cv_glmnet
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", lambda_glmnet = lasso$lambda.min, alpha=0.5)
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", lambda_glmnet = lambda.min, alpha=0.5)
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", lambda_glmnet = lasso$lambda.min, alpha=0.5)
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", alpha=0.5)
cv_glmnet
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", alpha=1,family="binomial")
update(packages="TANDEM")
log_reg$beta
log_reg$beta(2000)
log_reg$beta[2000]
log_reg$beta[beta != 0]
beta<-log_reg$beta
beta[beta!=0]
library(FactoMineR)
X<-load("..\\alon.rda")
res.pca<-PCA(x,scale.unit=TRUE,ncp=61,graph=TRUE)
#Computing the loadings with a command found on the following link :
#https://groups.google.com/g/factominer-users/c/BRN8jRm-_EM?pli=1
loadings<-sweep(res.pca$var$coord,2,sqrt(res.pca$eig[,1]),FUN="/")
#Now computing the matrix of principal components
T<-(x-colMeans(x))%*%loadings
#Exporting T and the loadings as csv files
write.csv(T,file="principal_components.csv")
write.csv(loadings,file="loadings.csv")
#Retrieving individual variance first and then cumulative variance
indiv_var<-res.pca$eig[,2]
cum_var<-res.pca$eig[,3]
#Moving on to plotting
library(ggplot2)
r<-c(1:61)
df<-data.frame(r,indiv_var,cum_var)
g_ind<-ggplot(df) +
geom_line(aes(r,indiv_var),size=1,color="red") +
xlab("Principal components") +
ylab("Variance")
g_cum<-ggplot(df) +
geom_line(aes(x,cum_var),size=1,color="blue") +
xlab("Principal components") +
ylab("Cumulatives variance")
#Projecting onto the first 2 principal components
P<-T[,1:2]
cl<-c(1:62)*0
for (i in 1:61) {
if (y[i]=="Tumour")
cl[i]<-1
}
p<-ggplot(as.data.frame(T)) + geom_point(aes(x=P[,1], y=P[,2],color=cl))
p<-p+xlab("PC1")+ylab("PC2")
res.pca$eig
X<-load("..\\alon.rda")
library(MASS)
dat<-vector()
for (i in 1:2000) {
dat<-cbind(dat,x[,i])
}
dat<-cbind(dat,y)
dat<-as.data.frame(dat)
C<-cov(x)
lin_discr<-lda(formula=y~x)
write.csv(lin_discr$means,file="LDA means.csv")
write.csv(C,file="cov.csv")
library(glmnet)
set.seed(7)
lasso<-cv.glmnet(x,y,alpha=1,family="binomial",nfolds = 62)
log_reg<-glmnet(x,y,alpha=1,family="binomial",lambda=lasso$lambda.min)
#To estimate CV errors
library(ipred)
# force predict to return class labels only
mypredict.lda <- function(object, newdata)
predict(object, newdata = newdata)$class
err_lda<-errorest(data=dat,y~x,model=lda,estimator="cv",predict=mypredict.lda,
est.para=control.errorest(k=62))
mypredict.logreg <- function(object, newdata)
predict.glmnet(object,newdata = newdata)$class
err_lda<-errorest(data=dat,y~x,model=lda,estimator="cv",predict=mypredict.lda,
est.para=control.errorest(,predictions=TRUE))
#moving on to logistic regression, code inspired by link below
# https://stackoverflow.com/questions/36121171/how-to-extract-actual-classification-error-rate-with-cost-function-from-cv-glmne
overall_error<-0
tumour_error<-0
normal_error<-0
folds<-c(1:62)
predictions <- c(1:62)*0
for (k in 1:62) {
x_train<-dat[-folds[k],1:2000]
x_test <-dat[folds[k],1:2000]
y_train<-dat[-folds[k],2001]
y_test<-dat[folds[k],2001]
fit<-glmnet(as.matrix(x_train),as.matrix(y_train),alpha=1,family="binomial",lambda=lasso$lambda.min)
pred<-predict(fit,as.matrix(x_test),s=lasso$lambda.min,type="class")
if (k < 41) {
if (pred!=dat$y[k]) {
tumour_error<-tumour_error+1
overall_error<-overall_error+1
}
}
else {
if (pred!=dat$y[k]) {
normal_error<-normal_error+1
overall_error<-overall_error+1
}
}
}
overall_error<-overall_error/62
tumour_error<-tumour_error/40
normal_error<-normal_error/22
#Using nested cv to determine lambda
library(TANDEM)
upstream<-vector(length=2000)
for (i in 1:2000) {
upstream[i]<-TRUE
}
cv_glmnet = nested.cv(x, dat$y, upstream=upstream, method="glmnet", alpha=1,family="binomial")
log_reg$beta[log_reg$beta != 0]
library(FactoMineR)
X<-load("..\\alon.rda")
res.pca<-PCA(x,scale.unit=TRUE,ncp=61,graph=TRUE)
#Computing the loadings with a command found on the following link :
#https://groups.google.com/g/factominer-users/c/BRN8jRm-_EM?pli=1
loadings<-sweep(res.pca$var$coord,2,sqrt(res.pca$eig[,1]),FUN="/")
#Now computing the matrix of principal components
T<-(x-colMeans(x))%*%loadings
#Exporting T and the loadings as csv files
write.csv(T,file="principal_components.csv")
write.csv(loadings,file="loadings.csv")
#Retrieving individual variance first and then cumulative variance
indiv_var<-res.pca$eig[,2]
cum_var<-res.pca$eig[,3]
#Moving on to plotting
library(ggplot2)
r<-c(1:61)
df<-data.frame(r,indiv_var,cum_var)
g_ind<-ggplot(df) +
geom_line(aes(r,indiv_var),size=1,color="red") +
xlab("Principal components") +
ylab("Variance")
g_cum<-ggplot(df) +
geom_line(aes(x,cum_var),size=1,color="blue") +
xlab("Principal components") +
ylab("Cumulatives variance")
#Projecting onto the first 2 principal components
P<-T[,1:2]
cl<-c(1:62)*0
for (i in 1:61) {
if (y[i]=="Tumour")
cl[i]<-1
}
p<-ggplot(as.data.frame(T)) + geom_point(aes(x=P[,1], y=P[,2],color=cl))
p<-p+xlab("PC1")+ylab("PC2")
X<-load("..\\alon.rda")
library(stats)
library(rstatix)
fdr<-0.01
dat<-as.data.frame(cbind(x,y))
gene_order<-c(1:2000)
orig_p_vals<-c(1:2000)*0
adj_p_vals<-c(1:2000)*0
for (i in 1:2000) {
temp<-data.frame(x=dat[,i],y=dat$y)
orig_p_vals[i]<-t_test(data=temp,formula=x~y,var.equal=FALSE,conf.level=1-fdr,
p.adjust.method = "BH")$p
}
p_vals<-data.frame(order=gene_order,p_value=orig_p_vals)
p_vals<-p_vals[order(p_vals$p_value),]
i<-1
threshold<-0
while (p_vals$p_value[i] < i*(fdr/2000)) {
threshold<-i*(fdr/2000)
i<-i+1
}
write.csv(p_vals[1:i,],file="Genes_declared_significant.csv")
library(ggplot2)
p<-ggplot(p_vals[1:i,])+geom_point(aes(sort(p_vals$order[1:i]),p_vals$p_value[1:i])) +
geom_vline(xintercept = i,linetype="dashed")+
xlab("Genes ordered by p-value")+ ylab("p-values")
threshold
